{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from numpy.ma.core import argmax\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ],
   "id": "c8d593cc402be67f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "def compute_class_frequencies(label_image_paths, num_classes):\n",
    "    \"\"\"\n",
    "    Compute class frequencies for each pixel location based on label images.\n",
    "\n",
    "    Parameters:\n",
    "        label_image_paths (list): List of file paths to the label images.\n",
    "        num_classes (int): Total number of classes.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Class probabilities of shape (height, width, num_classes).\n",
    "    \"\"\"\n",
    "    # Load the first label image to determine the dimensions\n",
    "    sample_image = Image.open(label_image_paths[0])\n",
    "    height, width = sample_image.size[::-1]  # PIL gives size as (width, height)\n",
    "\n",
    "    # Initialize an array to store class counts\n",
    "    class_counts = np.zeros((height, width, num_classes))\n",
    "\n",
    "    # Iterate over all label image paths\n",
    "    for path in tqdm(label_image_paths, desc=\"Computing class frequencies\"):\n",
    "        # Load the label image as a NumPy array\n",
    "        label = np.array(Image.open(path))\n",
    "\n",
    "        # Update class counts for each class\n",
    "        for c in range(num_classes):\n",
    "            class_counts[:, :, c] += (label == c)\n",
    "\n",
    "    # Normalize counts to get probabilities\n",
    "    class_probabilities = class_counts / len(label_image_paths)\n",
    "    return class_probabilities"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "from glob import glob\n",
    "\n",
    "# Assume label images are stored in a directory\n",
    "label_image_paths = glob(\"../amos22/Train/label/*.png\")  # Adjust file extension as needed\n",
    "num_classes = 16  # Example number of classes\n",
    "\n",
    "class_probabilities = compute_class_frequencies(label_image_paths, num_classes)\n",
    "class_probabilities"
   ],
   "id": "f5fe1416edfb3c87",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing class frequencies: 100%|██████████| 26283/26283 [11:21<00:00, 38.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T14:37:43.275586Z",
     "start_time": "2025-01-14T14:37:43.270765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_baseline(test_label_paths, class_probabilities, num_classes):\n",
    "    \"\"\"\n",
    "    Evaluate the baseline performance using computed class probabilities.\n",
    "\n",
    "    Parameters:\n",
    "        test_label_paths (list): List of file paths to the test label images.\n",
    "        class_probabilities (np.ndarray): Array of shape (height, width, num_classes).\n",
    "        num_classes (int): Total number of classes.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics (pixel accuracy, mIoU, F1-score).\n",
    "    \"\"\"\n",
    "    total_pixels = 0\n",
    "    correct_pixels = 0\n",
    "    iou_sum = np.zeros(num_classes)\n",
    "    true_positives = np.zeros(num_classes)\n",
    "    false_positives = np.zeros(num_classes)\n",
    "    false_negatives = np.zeros(num_classes)\n",
    "\n",
    "    for path in tqdm(test_label_paths, desc=\"Evaluating test data\"):\n",
    "        # Load the test label image\n",
    "        ground_truth = np.array(Image.open(path))\n",
    "\n",
    "        # Get the predicted segmentation map\n",
    "        predicted_map = np.argmax(class_probabilities, axis=-1)\n",
    "\n",
    "        # Flatten ground truth and prediction maps for metrics\n",
    "        gt_flat = ground_truth.flatten()\n",
    "        pred_flat = predicted_map.flatten()\n",
    "\n",
    "        # Compute metrics\n",
    "        total_pixels += len(gt_flat)\n",
    "        correct_pixels += np.sum(gt_flat == pred_flat)\n",
    "\n",
    "        for c in range(num_classes):\n",
    "            true_positives[c] += np.sum((gt_flat == c) & (pred_flat == c))\n",
    "            false_positives[c] += np.sum((gt_flat != c) & (pred_flat == c))\n",
    "            false_negatives[c] += np.sum((gt_flat == c) & (pred_flat != c))\n",
    "\n",
    "    # Pixel accuracy\n",
    "    pixel_accuracy = correct_pixels / total_pixels\n",
    "\n",
    "    # Compute IoU for each class and mean IoU\n",
    "    iou_sum = true_positives / (true_positives + false_positives + false_negatives + 1e-10)\n",
    "    mean_iou = np.mean(iou_sum)\n",
    "\n",
    "    # Compute F1-score for each class\n",
    "    f1_scores = 2 * true_positives / (2 * true_positives + false_positives + false_negatives + 1e-10)\n",
    "\n",
    "    return {\n",
    "        \"Pixel Accuracy\": pixel_accuracy,\n",
    "        \"Mean IoU\": mean_iou,\n",
    "        \"Class-wise IoU\": iou_sum,\n",
    "        \"Class-wise F1-scores\": f1_scores\n",
    "    }"
   ],
   "id": "677befa4d6af9471",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T14:39:40.428600Z",
     "start_time": "2025-01-14T14:37:43.287099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "test_label_paths = glob(\"../amos22/Validation/label/*.png\")  # Adjust path\n",
    "test_label_paths = test_label_paths[:200]\n",
    "metrics = evaluate_baseline(test_label_paths, class_probabilities, num_classes)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Baseline Metrics:\")\n",
    "print(f\"Pixel Accuracy: {metrics['Pixel Accuracy']:.4f}\")\n",
    "print(f\"Mean IoU: {metrics['Mean IoU']:.4f}\")\n",
    "print(\"Class-wise IoU:\\n\", metrics[\"Class-wise IoU\"])\n",
    "print(\"Class-wise F1-scores:\\n\", metrics[\"Class-wise F1-scores\"])"
   ],
   "id": "caabe1b18971ecdc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test data: 100%|██████████| 6481/6481 [01:57<00:00, 55.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Metrics:\n",
      "Pixel Accuracy: 0.9542\n",
      "Mean IoU: 0.0596\n",
      "Class-wise IoU:\n",
      " [0.95416275 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Class-wise F1-scores:\n",
      " [0.97654379 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T14:39:40.443306Z",
     "start_time": "2025-01-14T14:39:40.438567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_baseline_top_k_metrics(test_label_paths, class_probabilities, num_classes, top_k=3):\n",
    "    \"\"\"\n",
    "    Evaluate the baseline performance using top-k metrics for all relevant metrics.\n",
    "\n",
    "    Parameters:\n",
    "        test_label_paths (list): List of file paths to the test label images.\n",
    "        class_probabilities (np.ndarray): Array of shape (height, width, num_classes).\n",
    "        num_classes (int): Total number of classes.\n",
    "        top_k (int): The k value for top-k metrics.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing top-k evaluation metrics (pixel accuracy, mIoU, F1-score).\n",
    "    \"\"\"\n",
    "    total_pixels = 0\n",
    "    correct_top_k_pixels = 0\n",
    "    true_positives = np.zeros(num_classes)\n",
    "    false_positives = np.zeros(num_classes)\n",
    "    false_negatives = np.zeros(num_classes)\n",
    "\n",
    "    for path in tqdm(test_label_paths, desc=\"Evaluating test data\"):\n",
    "        # Load the test label image\n",
    "        ground_truth = np.array(Image.open(path))\n",
    "\n",
    "        # Get top-k predictions\n",
    "        top_k_predictions = np.argsort(-class_probabilities, axis=-1)[..., :top_k]\n",
    "\n",
    "        # Flatten ground truth and top-k predictions for metrics\n",
    "        gt_flat = ground_truth.flatten()\n",
    "        top_k_flat = top_k_predictions.reshape(-1, top_k)\n",
    "\n",
    "        total_pixels += len(gt_flat)\n",
    "\n",
    "        # Top-k accuracy\n",
    "        correct_top_k_pixels += np.sum([gt in top_k for gt, top_k in zip(gt_flat, top_k_flat)])\n",
    "\n",
    "        # Compute true positives, false positives, and false negatives\n",
    "        for c in range(num_classes):\n",
    "            # Check if the true class (c) is in the top-k predictions\n",
    "            is_true_class_in_top_k = [(gt == c) and (c in top_k) for gt, top_k in zip(gt_flat, top_k_flat)]\n",
    "            true_positives[c] += sum(is_true_class_in_top_k)\n",
    "\n",
    "            # False positives: Class c predicted in top-k but not the true class\n",
    "            is_false_positive = [(c in top_k) and (gt != c) for gt, top_k in zip(gt_flat, top_k_flat)]\n",
    "            false_positives[c] += sum(is_false_positive)\n",
    "\n",
    "            # False negatives: True class c not in top-k predictions\n",
    "            is_false_negative = [(gt == c) and (c not in top_k) for gt, top_k in zip(gt_flat, top_k_flat)]\n",
    "            false_negatives[c] += sum(is_false_negative)\n",
    "\n",
    "    # Compute Top-k Accuracy\n",
    "    top_k_accuracy = correct_top_k_pixels / total_pixels\n",
    "\n",
    "    # Compute IoU for each class and mean IoU\n",
    "    iou_per_class = true_positives / (true_positives + false_positives + false_negatives + 1e-10)\n",
    "    mean_iou = np.mean(iou_per_class)\n",
    "\n",
    "    # Compute F1-score for each class\n",
    "    f1_per_class = 2 * true_positives / (2 * true_positives + false_positives + false_negatives + 1e-10)\n",
    "\n",
    "    return {\n",
    "        \"Top-k Accuracy\": top_k_accuracy,\n",
    "        \"Mean IoU (Top-k)\": mean_iou,\n",
    "        \"Class-wise IoU (Top-k)\": iou_per_class,\n",
    "        \"Class-wise F1-scores (Top-k)\": f1_per_class\n",
    "    }"
   ],
   "id": "6e6535d122d10361",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-14T18:01:13.691740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "test_label_paths = glob(\"../amos22/Test/label/*.png\")  # Adjust path\n",
    "test_label_paths = test_label_paths[:200]\n",
    "top_k = 3  # Check top-3 predictions\n",
    "metrics = evaluate_baseline_top_k_metrics(test_label_paths, class_probabilities, num_classes, top_k=top_k)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Baseline Metrics (Top-k):\")\n",
    "print(f\"Top-{top_k} Accuracy: {metrics['Top-k Accuracy']:.4f}\")\n",
    "print(f\"Mean IoU (Top-{top_k}): {metrics['Mean IoU (Top-k)']:.4f}\")\n",
    "print(\"Class-wise IoU (Top-k):\", metrics[\"Class-wise IoU (Top-k)\"])\n",
    "print(\"Class-wise F1-scores (Top-k):\", metrics[\"Class-wise F1-scores (Top-k)\"])"
   ],
   "id": "80f8f5d210f01e15",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test data:   5%|▌         | 10/200 [03:18<1:03:05, 19.92s/it]"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
